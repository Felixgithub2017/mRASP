src: en
tgt: fr
model_arch: transformer_wmt_en_de_big
encoder_learned_pos: true
decoder_learned_pos: true
data_path: /data00/home/panxiao.94/experiments/pmnmt/experiments/toy/data/fine-tune/en2fr
model_dir: /data00/home/panxiao.94/experiments/pmnmt/experiments/toy/models/fine-tune/transformer_big/en2fr
pretrain_model_dir: /data00/home/panxiao.94/experiments/pmnmt/experiments/toy/models/pre-train/transformer_big
# 8 cards * 4 update_freq
update_freq: 4
log_interval: 5
save_interval_updates: 200
max_update: 40000
max_tokens: 11000
max_source_positions: 300
max_target_positions: 300
lr: 0.00005
dropout: 0.1
activation_fn: gelu
criterion: label_smoothed_cross_entropy
reset_optimizer: true
reset_lr_scheduler: true
reset_dataloader: true
reset_meters: true
lr_scheduler: inverse_sqrt
weight_decay: 0.0
clip_norm: 0.0
warmup_init_lr: 1e-07
label_smoothing: 0.1
fp16: true
seed: 9823843
